{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIKrzNuZOht72J/slZDjJT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tusharsharm/MDSC-302/blob/main/24040208008_MDSC_302_DEEP_LEARNING_ASSIGNMENT_IV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24040208008 MDSC-302 DEEP LEARNING ASSIGNMENT-IV"
      ],
      "metadata": {
        "id": "2n082HXvEkQ0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad854b87"
      },
      "source": [
        "# Analyze a large Python source code file by:\n",
        "- a) building a word index using simple tokenization,\n",
        "-  b) comparing its size to an index of \"Pride and Prejudice\",\n",
        "-  c) creating a one-hot encoding,\n",
        "-  d) discussing the information lost in the encoding compared to the \"Pride and Prejudice\" encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae836abc"
      },
      "source": [
        "## Select a python source code file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3292d759"
      },
      "source": [
        "## Tokenize the source code\n",
        "\n",
        "### Subtask:\n",
        "Implement a tokenization process for the selected source code file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c6c40e1"
      },
      "source": [
        "**Reasoning**:\n",
        "Read the content of the specified Python file, define a tokenization function, and apply it to the file content to get a list of tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dc4c635",
        "outputId": "8b244041-54fe-4d1d-a355-8d7753541ce6"
      },
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "file_path = \"/content/large_python_file.py\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    file_content = f.read()\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    # Convert to lower case\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and split into words\n",
        "    tokens = re.sub(r\"[^a-zA-Z0-9_]+\", \" \", text).split()\n",
        "    return tokens\n",
        "\n",
        "tokens = simple_tokenize(file_content)\n",
        "index = defaultdict(list)\n",
        "for pos, token in enumerate(tokens):\n",
        "    index[token].append(pos)\n",
        "\n",
        "# Print stats\n",
        "print(f\"Unique words in file: {len(index)}\")\n",
        "print(f\"Total tokens: {len(tokens)}\")\n",
        "print(f\"First 10 tokens: {tokens[:10]}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in file: 96\n",
            "Total tokens: 267\n",
            "First 10 tokens: ['import', 'os', 'import', 'sys', 'import', 'time', 'def', 'factorial', 'n', 'if']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5cea1d7"
      },
      "source": [
        "## Compare indices\n",
        "\n",
        "\n",
        "Compare the size of the index from the source code with an index created for \"Pride and Prejudice\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3592e17",
        "outputId": "e6ce6bc5-ddb9-4797-f568-dcff51129fc6"
      },
      "source": [
        "file_path = \"/content/1342-0.txt\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    file_content = f.read()\n",
        "\n",
        "pride_count = len(simple_tokenize(file_content))\n",
        "code_count = len(index)\n",
        "\n",
        "if code_count > pride_count:\n",
        "    print(f\"Python code has more unique tokens.{code_count} > {pride_count}\")\n",
        "else:\n",
        "    print(f\"Pride and Prejudice has more unique tokens.{pride_count} > {code_count}\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pride and Prejudice has more unique tokens.126078 > 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7d6e6d5"
      },
      "source": [
        "## Create one-hot encoding\n",
        "\n",
        "Generate the one-hot encoding for the source code file based on the created index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0024bdfb",
        "outputId": "34fda685-fd69-402e-b2bb-3a2c87e6ad3f"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Map each token to an index\n",
        "word_to_idx = {word: i for i, word in enumerate(index.keys())}\n",
        "\n",
        "# Create one-hot vectors for each token in order\n",
        "vocab_size = len(word_to_idx)\n",
        "one_hot_matrix = np.zeros((len(tokens), vocab_size), dtype=int)\n",
        "print(one_hot_matrix,\"\\n\")\n",
        "for i, token in enumerate(tokens):\n",
        "    one_hot_matrix[i, word_to_idx[token]] = 1\n",
        "\n",
        "print(one_hot_matrix.shape)  # (num_tokens, vocab_size)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]] \n",
            "\n",
            "(267, 96)\n"
          ]
        }
      ]
    }
  ]
}